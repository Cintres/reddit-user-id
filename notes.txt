Stylometry:
-Most systems based on lexical statistics
-Common words (stop words) are actually more interesting
than less frequent topical words

-Writer invariant: property of a text which is invariant of its author
  common properties: dist. of word lengths, sentence length,
  avg word length, vocab richness, frequency of function words,
  vocab richness, etc.

  -avg. sentence length and avg. word length not very useful
  -best is analysis of function words b/c used by authors subconsciously

-One method: find frequency of (20-50) most common words in anonymous text.
      The 50 dimensional vectors are then flattened into a plane with PCA.
      If two works are placed on the same plane, they are likely to have
      the same author.

-Rare Pairs: technique for identifying style based on collocation.
  collocation: word sequences appear more often than just by chance

-Punctuation use a good indicator as well

1.)
Mendenhall's Characteristic Curves of Composition
  -def: word length distributions will look the same among different works of authors
  -implementation: plot the word frequencies for each given user

2.)
Kilgariff's Chi-Squared Method
    -def: most commonly used words should be used in similar frequencies
    implementation: -Oversample the anonymous text to be proportionate to the known user's corpus
                    -combine the corpora of the anonymous user with each user, one by one.
                    -Take the n most common words (n is usually 100-1000, use smaller n if
                    you have a smaller document. Or use stop words.)
                    -Calculate Ei: how frequent the word should have come up in the anonymous text
                    by dividing each frequency of n words by the size of the known corpus
                    -calculate chi-squared for each token: x^2 = sum((Ci-Ei)^2)/Ei
                    -the smaller the chi value, the more similar the two corpora

3. )
Burrow's Delta Method
    -def: measure the distance between most common words of all corpora
    implementation: -Create a corpus from all users' corpora
                    -Find the n most frequent words in the corpus
                    -For each word, calculate frequency of that word/ total number in the user's subcorpus
                    -Turn each percent into a z score by subtracting the mean and dividing by std. dev
                    -Calculate delta score by taking the average of the absolute values of differences between
                       the z scores when comparing two candidates (cosine actually will outperform)
                    -Use Wurzburg Distance (cosine distance) instead because the 'stylistic profile' of a user is better represented
                        by the pattern of over and under-utilized words rather than the magnitude

4. ) Gunning-Fog Readability Index
    -def: calculates reading level needed to understand text
    implementation: 0.4 * (avg. sentence length + 100 * compex word ratio)





Reddit Formatting: Generally follows Markdown
[](): link
* * or _ _: italic
** ** or __ __: bold
"    " (4 spaces): block code
#, ##, ###: Headline 1 , 2, 3
***: Horizontal Rule (horizontal line)


PinkMST Hierarchical Clustering using Spark







References:
https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python
https://academic.oup.com/dsh/article/32/suppl_2/ii4/3865676
